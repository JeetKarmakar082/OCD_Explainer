<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <!-- <meta name="viewport" content="width=device-width,initial-scale=1" /> -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Oral Cancer Detection â€” Complete Explainer (MobileNetV2 + Django)</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        /* Light theme styles */
        :root {
            --bg: #f6fbff;
            --panel: #ffffff;
            --muted: #546e7a;
            --accent: #0b6ef6;
            --glass: rgba(11, 110, 246, 0.06);
            --card: #ffffff
        }

        html,
        body {
            height: 100%;
            margin: 0;
            font-family: Inter, ui-sans-serif, system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial
        }

        body {
            background: linear-gradient(180deg, #f6fbff 0%, #eaf4ff 100%);
            color: #102a43
        }

        header {
            padding: 26px 28px;
            border-bottom: 1px solid rgba(16, 42, 67, 0.05);
            display: flex;
            align-items: center;
            gap: 16px
        }

        header h1 {
            margin: 0;
            font-size: 20px
        }

        header p {
            margin: 0;
            color: var(--muted);
            font-size: 13px
        }

        .container {
            display: grid;
            grid-template-columns: 320px 1fr;
            gap: 18px;
            padding: 18px
        }

        nav {
            background: var(--panel);
            border-radius: 12px;
            padding: 14px;
            height: calc(100vh - 120px);
            overflow: auto;
            box-shadow: 0 6px 20px rgba(16, 42, 67, 0.06)
        }

        nav h2 {
            font-size: 13px;
            margin: 0 0 10px 0;
            color: var(--muted)
        }

        nav ul {
            list-style: none;
            padding: 0;
            margin: 0
        }

        nav li {
            margin: 8px 0
        }

        nav a {
            display: block;
            padding: 8px;
            border-radius: 8px;
            color: #0b6ef6;
            text-decoration: none;
            font-weight: 600
        }

        nav a:hover {
            background: var(--glass)
        }

        main {
            padding: 18px;
            background: transparent;
            min-height: 80vh;
            overflow: auto
        }

        .card {
            background: var(--card);
            border-radius: 12px;
            padding: 18px;
            margin-bottom: 14px;
            border: 1px solid rgba(16, 42, 67, 0.04);
            box-shadow: 0 6px 18px rgba(16, 42, 67, 0.03)
        }

        h2.section {
            font-size: 18px;
            margin: 0 0 8px 0;
            color: #0b6ef6
        }

        .muted {
            color: var(--muted);
            font-size: 18px;
        }

        pre.code {
            background: #0b1726;
            color: #e6f1ff;
            padding: 18px;
            border-radius: 10px;
            overflow: auto;
            font-size: 13px;
            line-height: 1.45
        }

        pre.light {
            background: #f3f7fb;
            padding: 18px;
            border-radius: 10px;
            border: 1px solid rgba(16, 42, 67, 0.03);
            overflow: auto
        }

        code.c {
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace
        }

        .kbd {
            background: #ebf4ff;
            padding: 4px 8px;
            border-radius: 6px;
            border: 1px solid rgba(11, 110, 246, 0.08)
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 10px
        }

        .btn {
            background: var(--accent);
            padding: 8px 12px;
            border-radius: 8px;
            border: none;
            color: white;
            cursor: pointer
        }

        .copy-btn {
            background: transparent;
            border: 1px solid rgba(16, 42, 67, 0.06);
            padding: 6px 8px;
            border-radius: 6px;
            color: var(--muted);
            cursor: pointer
        }

        .note {
            background: linear-gradient(90deg, rgba(11, 110, 246, 0.06), rgba(6, 182, 212, 0.03));
            padding: 10px;
            border-radius: 8px;
            color: #05386b
        }

        .small {
            font-size: 13px
        }

        footer {
            padding: 12px 28px;
            color: var(--muted);
            font-size: 13px
        }

        /* Syntax coloring classes for displayed code */
        .kw {
            color: #0b6ef6;
            font-weight: 600
        }

        .cmt {
            color: #0bf76a
        }

        .str {
            color: #eb0e57
        }

        .num {
            color: #e2f103
        }

        .fn {
            color: #805ad5
        }

        .op {
            color: #102a43
        }

        .typ {
            color: #2b6cb0
        }

        .cblk {
            background: #f3f7fb;
            padding: 12px;
            border-radius: 8px;
            border: 1px solid rgba(16, 42, 67, 0.03);
            color: #102a43
        }

        .section-sub {
            color: #0b6ef6;
            margin-top: 8px
        }

        .highlight {
            background: #fff5d6;
            padding: 6px;
            border-radius: 6px;
            border: 1px solid rgba(16, 42, 67, 0.03);
            display: block
        }

        /* Responsive fixes for small screens */
        @media (max-width: 768px) {

            body,
            html {
                overflow-x: hidden;
            }

            .card,
            .section,
            .container {
                width: 95%;
                margin: 10px auto;
                padding: 12px;
                box-sizing: border-box;
            }

            h1 {
                font-size: 1.6em !important;
                text-align: center;
            }

            h2 {
                font-size: 1.3em !important;
            }

            h3 {
                font-size: 1.1em !important;
            }

            p,
            li {
                font-size: 0.95em !important;
                line-height: 1.5em;
            }

            img,
            video,
            iframe,
            canvas {
                max-width: 100%;
                height: auto;
                display: block;
                margin: auto;
            }

            pre,
            code {
                max-width: 100%;
                overflow-x: auto;
                white-space: pre-wrap;
                word-break: break-word;
                font-size: 0.9em;
            }

            button,
            .btn,
            .copy-btn {
                width: 100%;
                margin: 6px 0;
                font-size: 1em;
                padding: 10px;
                box-sizing: border-box;
            }

            input[type="range"] {
                width: 100%;
            }

            /* Adjust slider container */
            .slider-container {
                width: 100%;
                display: flex;
                flex-direction: column;
                align-items: center;
            }

            .output-box {
                font-size: 1em;
                margin-top: 8px;
                text-align: center;
            }
        }
    </style>
</head>

<body>
    <header>
        <div
            style="width:52px;height:52px;border-radius:10px;background:linear-gradient(135deg,#0b6ef6,#06b6d4);display:flex;align-items:center;justify-content:center;font-weight:700;color:white">
            AI</div>
        <div>
            <h1>Oral Cancer Detection â€” Complete Explainer</h1>
            <p class="muted">Everything from concept to code â€” full detailed explanations of the project.</p>
            <p class="muted">Made for Anusuya , Debanka, Jeet, Srijita</p>
        </div>
    </header>

    <div class="container">
        <nav>
            <h2>Contents</h2>
            <ul>
                <li><a href="#overview">Overview & Objectives</a></li>
                <li><a href="#background">Domain Background</a></li>
                <li><a href="#pipeline">Image Processing Pipeline</a></li>
                <li><a href="#mobilenet">MobileNetV2 Deep Dive</a></li>
                <li><a href="#codeblock">Colorized Training Script (full)</a></li>
                <li><a href="#code-explanations">Code â€” Line-by-line Explanations</a></li>
                <li><a href="#trainstrategy">Training Strategy & Hyperparams</a></li>
                <li><a href="#evaluation">Evaluation & Metrics</a></li>
                <li><a href="#deployment">Deployment & TFLite</a></li>
                <li><a href="#vivaqa">Viva Q&A</a></li>
                <li><a href="#interactive">Interactive Playground</a></li>
                <li><a href="#resources">Resources & Next Steps</a></li>
            </ul>
        </nav>

        <main>

            <section id="overview" class="card">
                <h2 class="section">Overview & Objectives</h2>
                <p>This comprehensive page explain <strong>everything</strong> about your
                    Oral Cancer Detection project. It includes:</p>
                <ul class="muted">
                    <li>Domain background and rationale</li>
                    <li>Detailed image preprocessing pipeline with tensor shapes</li>
                    <li>MobileNetV2 internals and intuition</li>
                    <li>Complete, colorized training script with <em>all</em> comments preserved</li>
                    <li>Line-by-line explanations of the script and choices</li>
                    <li>Training strategy, evaluation, deployment, viva prep</li>
                    <li>Interactive playground to visualize preprocessing and simulate model output</li>
                </ul>
            </section>

            <section id="background" class="card">
                <h2 class="section">Domain Background â€” Oral Cancer & IG-PDT</h2>
                <h3 class="section-sub">What is Oral Cancer?</h3>
                <p>Oral cancer arises in tissues of the mouth or throat. Common sites include tongue, floor of the
                    mouth, buccal mucosa, palate, and lips. Risk factors include tobacco (smoking and smokeless),
                    alcohol, HPV infection, and certain dietary or genetic factors. Early detection drastically improves
                    survival rates.</p>

                <h3 class="section-sub">Photodynamic Therapy (PDT)</h3>
                <p>PDT uses a photosensitizer drug which preferentially accumulates in cancer cells. When illuminated
                    with a specific wavelength of light, the drug produces reactive oxygen species that kill cancer
                    cells. PDT is minimally invasive and preserves surrounding healthy tissue when applied correctly.
                </p>

                <h3 class="section-sub">Image-Guided Photodynamic Therapy (IG-PDT)</h3>
                <p>IG-PDT uses imaging (cameras, endoscopes) and ML-based analysis to identify lesion boundaries and
                    guide light delivery precisely. Your project focuses on the imaging + detection part: automatically
                    detect suspicious lesions to guide PDT.</p>
            </section>

            <section id="pipeline" class="card">
                <h2 class="section">Image Processing Pipeline â€” Full Technical Details</h2>
                <p>Below is the pipeline an uploaded image goes through in your Django + MobileNetV2 system. Each step
                    includes the reasons, expected data types/shapes, and common pitfalls.</p>

                <h4 class="section-sub">1. Upload</h4>
                <p>User uploads through a web form: <code>request.FILES['image']</code>. The file is typically saved
                    temporarily (e.g., <code>static/uploads/</code>) before processing.</p>

                <h4 class="section-sub">2. Load & Resize</h4>
                <pre class="light"><code class="c">from tensorflow.keras.preprocessing import image
img = image.load_img(img_path, target_size=(224, 224))  # PIL image</code></pre>
                <p><strong>Why:</strong> enforce consistent input size for model (MobileNetV2 expects 224Ã—224).
                    <strong>Pitfall:</strong> resizing can distort aspect ratio; for some tasks, preserve aspect by
                    padding instead.
                </p>

                <h4 class="section-sub">3. To NumPy Array</h4>
                <pre class="light"><code class="c">img_array = image.img_to_array(img)
# shape -> (224, 224, 3), dtype=float32 or uint8 depending on loader</code></pre>
                <p>Represents image as HÃ—WÃ—C array with values in 0..255.</p>

                <h4 class="section-sub">4. Normalization / Preprocessing</h4>
                <pre class="light"><code class="c">img_array = img_array / 255.0          # values now in [0,1]
# Alternatively, use MobileNetV2's preprocess_input for exact ImageNet scaling:
# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
# img_array = preprocess_input(img_array)</code></pre>
                <p><strong>Why:</strong> stabilizes gradients and matches what pretrained weights expect.
                    <strong>Note:</strong> using the library's <code>preprocess_input</code> is recommended for transfer
                    learning.
                </p>

                <h4 class="section-sub">5. Batch Dimension</h4>
                <pre
                    class="light"><code class="c">img_batch = np.expand_dims(img_array, axis=0)  # shape -> (1,224,224,3)</code></pre>
                <p>Represents a batch of size 1. Models accept batch inputs.</p>

                <h4 class="section-sub">6. Predict</h4>
                <pre
                    class="light"><code class="c">pred = model.predict(img_batch)  # returns array like [[0.23]]</code></pre>
                <p>Sigmoid output for binary classification. <code>0</code> meaning cancer (in your mapping),
                    <code>1</code> meaning normal. Threshold at 0.5 by default; you can tune threshold on validation set
                    for better sensitivity/ specificity trade-off.
                </p>

                <h4 class="section-sub">7. Interpret & Display</h4>
                <p>Convert probability to text and show to user. Example:
                    <code>if pred &lt; 0.5: result = f"Cancer detected ({(1-pred)*100:.2f}%)"</code>.
                </p>

                <h4 class="section-sub">Optional: Grad-CAM</h4>
                <p>Use Grad-CAM to compute gradients of target class w.r.t final conv layer, weigh feature maps, and
                    overlay a heatmap on the original image. Very useful for explainability in medical applications.</p>
            </section>

            <section id="mobilenet" class="card">
                <h2 class="section">MobileNetV2 â€” Architecture, Blocks & Intuition</h2>

                <h4 class="section-sub">Why MobileNetV2?</h4>
                <ul class="muted">
                    <li>Lightweight (â‰ˆ3.4M parameters) â€” suitable for servers and mobile.</li>
                    <li>Fast inference and lower memory usage.</li>
                    <li>Excellent transfer learning performance when dataset is small.</li>
                </ul>

                <h4 class="section-sub">Key concepts</h4>
                <p><strong>Depthwise Separable Convolution:</strong> split convolution into depthwise and pointwise
                    steps to reduce computation. <strong>Inverted Residual with Linear Bottleneck:</strong> expand â†’
                    depthwise â†’ project, using linear activation at projection to avoid information loss when
                    compressing features.</p>

                <h4 class="section-sub">Block pseudocode</h4>
                <pre class="light"><code class="c"># Input -> 1x1 conv (expand) -> 3x3 depthwise conv -> 1x1 conv (project linear)
# Add skip connection if shapes match</code></pre>

                <h4 class="section-sub">Activations</h4>
                <p>MobileNetV2 uses <code>ReLU6</code> in intermediate layers (good for quantization) and a linear
                    activation in the projection layer.</p>

                <h4 class="section-sub">Feature extraction flow</h4>
                <p>Early layers: edges & colors. Mid-layers: textures and shapes. Deep layers: high-level structures
                    distinguishing cancerous vs normal tissue.</p>
            </section>

            <section id="codeblock" class="card">
                <h2 class="section">Model Training Script â€” Comments for explaination</h2>

                <pre class="code"><code class="c"> <span class="cmt"># Standard numeric + deep learning libraries</span>
<span class="kw">import</span> numpy <span class="kw">as</span> np                              <span class="cmt"># numerical arrays, utilities</span>
<span class="kw">import</span> tensorflow <span class="kw">as</span> tf                         <span class="cmt"># main ML framework (Keras API included)</span>

<span class="cmt"># Utilities for loading/augmenting image data and building models</span>
<span class="kw">from</span> tensorflow.keras.preprocessing.image <span class="kw">import</span> ImageDataGenerator
<span class="kw">from</span> tensorflow.keras <span class="kw">import</span> layers, models
<span class="kw">from</span> tensorflow.keras.applications <span class="kw">import</span> MobileNetV2
<span class="kw">from</span> tensorflow.keras.optimizers <span class="kw">import</span> Adam

<span class="cmt"># Utilities for handling class imbalance and evaluation</span>
<span class="kw">from</span> sklearn.utils <span class="kw">import</span> class_weight
<span class="kw">from</span> sklearn.metrics <span class="kw">import</span> classification_report

<span class="cmt"># OS utilities (optional here, but often useful)</span>
<span class="kw">import</span> os

<span class="cmt"># -------------------------</span>
<span class="cmt"># Dataset paths (change these to match your folders)</span>
<span class="cmt"># expected directory structure:</span>
<span class="cmt"># dataset/</span>
<span class="cmt">#   train/</span>
<span class="cmt">#     cancer/</span>
<span class="cmt">#     normal/</span>
<span class="cmt">#   val/</span>
<span class="cmt">#     cancer/</span>
<span class="cmt">#     normal/</span>
train_dir = <span class="str">'dataset/train'</span>
val_dir   = <span class="str">'dataset/val'</span>

<span class="cmt"># -------------------------</span>
<span class="cmt"># Hyperparameters / constants</span>
IMG_SIZE   = (<span class="num">224</span>, <span class="num">224</span>)   <span class="cmt"># MobileNetV2 expects 224x224 (height, width)</span>
BATCH_SIZE = <span class="num">32</span>
EPOCHS     = <span class="num">20</span>           <span class="cmt"># not actively used later (script uses 10 + 10), but kept as a global reference</span>

<span class="cmt"># -------------------------</span>
<span class="cmt"># Data augmentation for training set:</span>
<span class="cmt"># ImageDataGenerator performs realtime augmentation during training.</span>
train_datagen = ImageDataGenerator(
    rescale=<span class="num">1.</span>/<span class="num">255</span>,        <span class="cmt"># scale pixel values from [0,255] to [0,1]</span>
    rotation_range=<span class="num">30</span>,     <span class="cmt"># randomly rotate images up to 30 degrees</span>
    zoom_range=<span class="num">0.3</span>,        <span class="cmt"># randomly zoom in/out up to 30%</span>
    width_shift_range=<span class="num">0.2</span>, <span class="cmt"># random horizontal shifts</span>
    height_shift_range=<span class="num">0.2</span>,<span class="cmt"># random vertical shifts</span>
    horizontal_flip=<span class="kw">True</span>,  <span class="cmt"># randomly flip images left-right</span>
    shear_range=<span class="num">0.2</span>,       <span class="cmt"># shear transformation</span>
    fill_mode=<span class="str">'nearest'</span>    <span class="cmt"># how to fill pixels after transform</span>
)

<span class="cmt"># For validation we only rescale â€” no augmentation</span>
val_datagen = ImageDataGenerator(rescale=<span class="num">1.</span>/<span class="num">255</span>)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Create generators that read images from the directory structure</span>
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,     <span class="cmt"># resize all images to 224x224</span>
    batch_size=BATCH_SIZE,
    class_mode=<span class="str">'binary'</span>,      <span class="cmt"># use 'binary' for two classes => yields a single sigmoid target</span>
    shuffle=<span class="kw">True</span>              <span class="cmt"># shuffle training data every epoch (recommended)</span>
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode=<span class="str">'binary'</span>,
    shuffle=<span class="kw">False</span>             <span class="cmt"># important: we set shuffle=False for consistent evaluation/predictions</span>
)

<span class="cmt"># Print mapping of class names to numeric labels e.g. {'cancer': 0, 'normal': 1}</span>
print(<span class="str">"Class indices:"</span>, train_generator.class_indices)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Compute class weights to handle class imbalance</span>
<span class="cmt"># train_generator.classes is a numpy array of labels for each sample in the training set</span>
class_weights = class_weight.compute_class_weight(
    class_weight=<span class="str">'balanced'</span>,
    classes=np.unique(train_generator.classes),  <span class="cmt"># e.g. [0, 1]</span>
    y=train_generator.classes
)
<span class="cmt"># convert to dict: {0: weight_for_class0, 1: weight_for_class1}</span>
class_weights_dict = dict(enumerate(class_weights))
print(<span class="str">"Class weights:"</span>, class_weights_dict)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Load MobileNetV2 base model (pretrained on ImageNet), exclude top</span>
base_model = MobileNetV2(
    input_shape=IMG_SIZE + (<span class="num">3</span>,),  <span class="cmt"># (224,224,3)</span>
    include_top=<span class="kw">False</span>,            <span class="cmt"># remove final FC layers â€” we'll add custom head</span>
    weights=<span class="str">'imagenet'</span>            <span class="cmt"># load ImageNet weights for transfer learning</span>
)

<span class="cmt"># Freeze all base model layers to train only the new head first</span>
base_model.trainable = <span class="kw">False</span>

<span class="cmt"># -------------------------</span>
<span class="cmt"># Build a simple classification head on top of the frozen base</span>
model = models.Sequential([
    base_model,                             <span class="cmt"># feature extractor</span>
    layers.GlobalAveragePooling2D(),        <span class="cmt"># convert feature maps to a single vector per image</span>
    layers.Dense(<span class="num">128</span>, activation=<span class="str">'relu'</span>),   <span class="cmt"># a small dense layer to learn task-specific combos</span>
    layers.Dropout(<span class="num">0.4</span>),                    <span class="cmt"># reduce overfitting</span>
    layers.Dense(<span class="num">1</span>, activation=<span class="str">'sigmoid'</span>)   <span class="cmt"># final output for binary classification (0..1)</span>
])

<span class="cmt"># Compile model with Adam optimizer and binary crossentropy loss</span>
model.compile(
    optimizer=Adam(learning_rate=<span class="num">0.0001</span>),
    loss=<span class="str">'binary_crossentropy'</span>,
    metrics=[<span class="str">'accuracy'</span>]
)

<span class="cmt"># Show model architecture and parameter counts</span>
model.summary()

<span class="cmt"># -------------------------</span>
<span class="cmt"># Train only the top layers (base_model frozen)</span>
history = model.fit(
    train_generator,
    epochs=<span class="num">10</span>,                 <span class="cmt"># train head for 10 epochs first</span>
    validation_data=val_generator,
    class_weight=class_weights_dict
)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Fine-tuning: unfreeze the last 30 layers of the base model</span>
base_model.trainable = <span class="kw">True</span>
for layer in base_model.layers[:-<span class="num">30</span>]:  <span class="cmt"># keep all but last 30 layers frozen</span>
    layer.trainable = <span class="kw">False</span>

<span class="cmt"># Recompile with a lower learning rate for fine-tuning</span>
model.compile(
    optimizer=Adam(learning_rate=<span class="num">1e-5</span>),
    loss=<span class="str">'binary_crossentropy'</span>,
    metrics=[<span class="str">'accuracy'</span>]
)

<span class="cmt"># Continue training (fine-tune)</span>
fine_tune_epochs = <span class="num">10</span>
total_epochs = <span class="num">10</span> + fine_tune_epochs  <span class="cmt"># initial 10 + fine-tune 10</span>

history_fine = model.fit(
    train_generator,
    epochs=total_epochs,
    initial_epoch=history.epoch[-<span class="num">1</span>],  <span class="cmt"># start from where previous training left off</span>
    validation_data=val_generator,
    class_weight=class_weights_dict
)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Save final model to disk</span>
model.save(<span class="str">'oral_cancer_mobilenetv2_finetuned.h5'</span>)
print(<span class="str">"Model saved as oral_cancer_mobilenetv2_finetuned.h5"</span>)

<span class="cmt"># -------------------------</span>
<span class="cmt"># Evaluate on validation set</span>
val_loss, val_acc = model.evaluate(val_generator)
print(f<span class="str">"Validation Accuracy: {val_acc:.4f}"</span>)

<span class="cmt"># Prepare true labels + predictions for a classification report</span>
val_generator.reset()                         <span class="cmt"># ensure generator order starts from beginning</span>
Y_true = val_generator.classes                <span class="cmt"># true labels in the same order (shuffle=False ensured this)</span>
Y_pred_probs = model.predict(val_generator)   <span class="cmt"># predicted probabilities for each image</span>
Y_pred = (Y_pred_probs &gt; <span class="num">0.5</span>).astype(<span class="kw">int</span>).reshape(-<span class="num">1</span>)  <span class="cmt"># convert to 0/1 labels</span>

<span class="cmt"># Print precision/recall/F1 per class</span>
print(classification_report(Y_true, Y_pred, target_names=list(train_generator.class_indices.keys())))

<span class="cmt"># Re-print class indices as a reminder</span>
print(<span class="str">"Class indices:"</span>, train_generator.class_indices)</code></pre>

                <div class="note" style="margin-top:12px">All comments are preserved exactly as in your script. The code
                    block above is copy-ready. Use the 'Copy script' button to paste it into your editor.</div>
            </section>

            <section id="code-explanations" class="card">
                <h2 class="section">Code â€” Detailed Line-by-Line Explanations</h2>
                <p>Below I expand each logical block from the script and explain the reasoning, expected values, shapes,
                    and common pitfalls.</p>

                <h4 class="section-sub">Imports</h4>
                <p><strong>numpy</strong> for numeric ops. <strong>tensorflow</strong> (with Keras API) for model
                    building & training. <strong>ImageDataGenerator</strong> for real-time augmentation.
                    <strong>class_weight</strong> handles imbalance. <strong>classification_report</strong> shows
                    precision/recall/F1.
                </p>

                <h4 class="section-sub">Dataset paths</h4>
                <p>Store images with structure: <code>dataset/train/cancer/*</code>, <code>dataset/train/normal/*</code>
                    and similarly for <code>val</code>. <strong>flow_from_directory</strong> maps folder names to labels
                    automatically.</p>

                <h4 class="section-sub">Hyperparameters</h4>
                <p><strong>IMG_SIZE</strong> must match model <code>input_shape</code>. <strong>BATCH_SIZE</strong>
                    depends on memory; reduce if running OOM. <strong>EPOCHS</strong> is a global reference; training
                    uses 10 + 10 split for head + fine-tune.</p>

                <h4 class="section-sub">Data augmentation</h4>
                <p>Augmentations artificially increase dataset variability â€” helpful for small datasets. Use
                    <code>preprocess_input</code> from MobileNetV2 if you want exact ImageNet normalization.
                </p>

                <h4 class="section-sub">Generators</h4>
                <p><code>shuffle=True</code> for training; <code>shuffle=False</code> for validation so that
                    <code>val_generator.classes</code> aligns with predictions. The generators yield tuples (X_batch,
                    y_batch) where X_batch has shape (batch, 224, 224, 3).
                </p>

                <h4 class="section-sub">Class weights</h4>
                <p>If one class is underrepresented, class weights increase its loss contribution. Example: 200 normal,
                    50 cancer â†’ class weight for cancer will be larger.</p>

                <h4 class="section-sub">MobileNetV2 loading</h4>
                <p>Using <code>include_top=False</code> removes ImageNet classifier so we can attach a custom head.
                    <code>weights='imagenet'</code> initializes with pretrained weights for transfer learning.
                </p>

                <h4 class="section-sub">Model head</h4>
                <p><code>GlobalAveragePooling2D</code> reduces parameters vs flattening. Dense(128) learns task-specific
                    patterns; Dropout(0.4) reduces overfitting; final Dense(1, sigmoid) outputs probability.</p>

                <h4 class="section-sub">Compilation</h4>
                <p>Binary crossentropy fits binary tasks with sigmoid output. Adam optimizer is a good default; learning
                    rate 1e-4 for head training, 1e-5 for fine-tuning.</p>

                <h4 class="section-sub">Training process</h4>
                <p>Stage 1: train head (base frozen) for 10 epochs. Stage 2: unfreeze last 30 layers and fine-tune with
                    small LR for 10 more epochs. Use callbacks to save best model and stop early if validation stops
                    improving.</p>

                <h4 class="section-sub">Saving & evaluation</h4>
                <p>Save with <code>model.save()</code>. When predicting on generator, call
                    <code>val_generator.reset()</code> first. <code>model.predict(val_generator)</code> returns
                    probabilities; threshold at 0.5 to get labels for <code>classification_report</code>.
                </p>

                <h4 class="section-sub">Pitfalls and fixes</h4>
                <ul class="muted">
                    <li>OOM: lower batch size or use mixed precision.</li>
                    <li>Label mismatch: ensure <code>shuffle=False</code> for val generator and build
                        <code>target_names</code> from <code>class_indices</code> mapping safely.
                    </li>
                    <li>Slow convergence: try different LR, increase augmentation, or unfreeze more layers.</li>
                </ul>
            </section>

            <section id="trainstrategy" class="card">
                <h2 class="section">Training Strategy & Practical Tips</h2>
                <p>Use callbacks:</p>
                <pre class="light"><code class="c">from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

checkpoint = ModelCheckpoint('best.h5', monitor='val_loss', save_best_only=True)
early = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)
reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)</code></pre>
                <p>Monitoring <code>val_loss</code> and <code>val_recall</code> (or custom metrics) is crucial. For
                    medical tasks prefer high recall (low false negatives).</p>
            </section>

            <section id="evaluation" class="card">
                <h2 class="section">Evaluation â€” Metrics & Reporting</h2>
                <p>Report accuracy, precision, recall, F1-score, confusion matrix, and ROC-AUC. Use class-level metrics
                    and show example images with Grad-CAM visualizations to justify predictions.</p>

                <pre class="light"><code class="c">from sklearn.metrics import confusion_matrix, roc_auc_score
cm = confusion_matrix(Y_true, Y_pred)
auc = roc_auc_score(Y_true, Y_pred_probs)</code></pre>
            </section>

            <section id="deployment" class="card">
                <h2 class="section">Deployment & Mobile Conversion</h2>
                <h4 class="section-sub">Django Integration</h4>
                <p>Load model at server start:
                    <code>model = tf.keras.models.load_model('oral_cancer_mobilenetv2_finetuned.h5')</code>. For each
                    upload, follow preprocessing steps and call <code>model.predict()</code>. Sanitize & limit file
                    sizes, and process asynchronously if necessary (but in our chat we cannot do background tasks â€”
                    process synchronously).
                </p>

                <h4 class="section-sub">TFLite Conversion</h4>
                <pre class="light"><code class="c">converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_dir')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open('model.tflite','wb').write(tflite_model)</code></pre>
                <p>Quantize for smaller size. For edge deployment, test accuracy drop after quantization.</p>

                <h4 class="section-sub">Privacy & Security</h4>
                <p>Store images securely, delete interim files, and get patient consent. Consider encrypting stored
                    images and limit retention.</p>
            </section>

            <section id="vivaqa" class="card">
                <h2 class="section">Viva Questions & Answers (Extended)</h2>
                <p>I've included an extended viva pack covering conceptual, technical, and implementation questions,
                    matching the earlier chat. If you want, I will expand each answer into a script you can memorize.
                </p>
                <ul class="muted">
                    <li><strong>What is IG-PDT?</strong> â€” explained earlier in domain background.</li>
                    <li><strong>Why MobileNetV2?</strong> â€” lightweight, fast, transfer learning suited.</li>
                    <li><strong>Explain the preprocessing steps.</strong> â€” load, resize, to array, normalize, batch,
                        predict, postprocess.</li>
                    <li><strong>How did you handle imbalance?</strong> â€” class_weight and augmentation; could use focal
                        loss or oversampling.</li>
                    <li><strong>How to evaluate clinically?</strong> â€” external validation with clinician-annotated
                        dataset, prospective study.</li>
                </ul>
            </section>

            <section id="interactive" class="card">
                <h2 class="section">Model Output Simulation</h2>
                <p>This section simulates how the MobileNetV2 model predicts oral cancer probability.
                    Move the slider to change the predicted value and see whether the model detects
                    <strong>Cancer</strong> or <strong>Normal Tissue</strong>.
                </p>

                <div class="cblk" style="margin-top:16px;max-width:400px">
                    <div class="muted">Simulated model probability</div>
                    <input id="predSlider" type="range" min="0" max="100" value="70" oninput="updatePred()"
                        style="width:100%">
                    <div style="margin-top:8px;font-size:16px">
                        <strong>Predicted Probability:</strong> <span id="predVal">0.70</span>
                    </div>
                    <div style="margin-top:10px;font-size:18px;font-weight:700" id="predLabel">Normal Tissue (70%)</div>
                    <small class="muted" style="display:block;margin-top:8px">
                        This slider mimics the model output (0 â†’ 1). Values under 0.5 indicate <strong>Cancer
                            Detected</strong>,
                        and values above 0.5 indicate <strong>Normal Tissue</strong>.
                    </small>
                </div>
            </section>



            <section id="resources" class="card">
                <h2 class="section">Resources & Next Steps</h2>
                <ul class="muted">
                    <li>MobileNetV2 paper â€” Sandler et al., 2018</li>
                    <li>TensorFlow Keras docs â€” models & preprocess_input</li>
                    <li>Grad-CAM libraries and implementations</li>
                    <li>How to create a clinical validation dataset and ethics considerations</li>
                </ul>
            </section>

        </main>
    </div>

    <script>

        // ===== Prediction Slider Simulation =====
        function updatePred() {
            const v = document.getElementById('predSlider').value;
            const p = (v / 100).toFixed(2);
            document.getElementById('predVal').innerText = p;

            let label = "";
            if (p < 0.5) {
                const cancerProb = ((1 - p) * 100).toFixed(1);
                label = `ðŸ§¬ Cancer Detected (${cancerProb}%)`;
                document.getElementById('predLabel').style.color = "#d62828"; // red
            } else {
                const normalProb = (p * 100).toFixed(1);
                label = `âœ… Normal Tissue (${normalProb}%)`;
                document.getElementById('predLabel').style.color = "#2a9d8f"; // green
            }
            document.getElementById('predLabel').innerText = label;
        }
        updatePred();

    </script>


</body>

</html>

